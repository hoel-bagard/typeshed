from collections.abc import Callable
from typing import Any
from typing_extensions import TypeAlias

from tensorflow import Tensor
from tensorflow._aliases import FloatDataSequence, FloatTensorCompatible, Integer

# The implementation uses isinstance so it must be dict and not any Mapping.
_Activation: TypeAlias = str | None | Callable[[Tensor], Tensor] | dict[str, Any]

def deserialize(
    name: str, custom_objects: dict[str, Callable[..., Any]] | None = None, use_legacy_format: bool = False
) -> Callable[..., Any]: ...
def elu(x: FloatTensorCompatible, alpha: FloatTensorCompatible | FloatDataSequence = 1.0) -> Tensor: ...
def exponential(x: FloatTensorCompatible) -> Tensor: ...
def gelu(x: FloatTensorCompatible, approximate: bool = False) -> Tensor: ...
def get(identifier: _Activation) -> Callable[[Tensor], Tensor]: ...
def hard_sigmoid(x: FloatTensorCompatible) -> Tensor: ...
def linear(x: FloatTensorCompatible) -> Tensor: ...
def mish(x: FloatTensorCompatible) -> Tensor: ...
def relu(
    x: FloatTensorCompatible | FloatDataSequence,
    alpha: FloatTensorCompatible = 0.0,
    max_value: FloatTensorCompatible | FloatDataSequence | None = None,
    threshold: FloatTensorCompatible | FloatDataSequence = 0.0,
) -> Tensor: ...
def selu(x: FloatTensorCompatible | FloatDataSequence) -> Tensor: ...  # x here cannot be an int.
def serialize(activation: Callable[..., Any], use_legacy_format: bool = False) -> str: ...
def sigmoid(x: FloatTensorCompatible | FloatDataSequence) -> Tensor: ...  # x here cannot be an int.
def softmax(x: Tensor, axis: Integer = -1) -> Tensor: ...
def softplus(x: FloatTensorCompatible | FloatDataSequence) -> Tensor: ...  # x here cannot be an int.
def softsign(x: FloatTensorCompatible | FloatDataSequence) -> Tensor: ...  # x here cannot be an int.
def swish(x: FloatTensorCompatible | FloatDataSequence) -> Tensor: ...  # x here cannot be an int.
def tanh(x: FloatTensorCompatible | FloatDataSequence) -> Tensor: ...  # x here cannot be an int.
